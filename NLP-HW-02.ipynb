{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоматическая обработка языка\n",
    "## НИУ ВШЭ, 2020-2021 учебный год\n",
    "\n",
    "### Домашнее задание №2\n",
    "\n",
    "Задание выполнил(а): Дарья Матяш\n",
    "\n",
    "Ссылка на условия и требования: https://github.com/named-entity/hse-nlp/blob/master/hometask%202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)В этой домашке вам будет нужно найти или самим написать русский и английский тексты (каждый от ста слов), в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их в ручную (а потом объяснить, какие моменты вы тут считаете трудными для автоматического посттеггинга и почему) – с помощью этих текстов мы будем оценивать качество работы наших теггоров. В текстах размечаем только части речи, ничего больше!\n",
    "Вы получаете **балл** за **создание**, **разметку текста** и **объяснение** того, почему этот текст подходит для оценки (что в нём сложного). Всего за этот пункт **2 балла, т. к. языка 2**.\n",
    "\n",
    "\n",
    "\n",
    "2)Потом вам будет нужно взять **три  POS теггера для русского (pymorphy2, mysteam, Natasha)** и **3  - для английского (SpyCy, Flair, NLTK)** и «прогнать» текст через каждый из них (если вы запустите только **2 теггера из трёх** – получите **балл**, если **три  из трёх** – **2 балла**, т. е. суммарно за этот пункт можно получить **4 балла**).\n",
    "\n",
    "\n",
    "3)После этого вам надо будет оценить **accuracy** для каждого теггера. Заметьте, что в разных системах имена теггов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции или кода и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции. Этот пункт стоит **2 балла**.\n",
    "\n",
    "\n",
    "Тут вы уже получили **8 баллов**.\n",
    "\n",
    "\n",
    "4)Дальше вам нужно взять **лучший теггер** для русского языка и  с его помощью написать функцию, которая **повысит качество работы программы** из первой домашки. Так, многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Вам надо выделить **3 вида синтаксических групп** (к примеру не + какая-то часть речи или NP или сущ.+ наречие или еще что-то), запись которых в словарь, по вашему мнению, улучшила бы качество работы программы и создать такую функцию или функции, которые с помощью любых известных нам средств (chunking и regexp grammar, Natasha syntax parser,  код с последнего занятия по SpyCy, etc.) будет выделять эти группы в поданном в нее тексте. **Два балла** за саму **функцию**, **балл** за **объяснение** того, почему именно эти группы вы взяли.\n",
    "***2 бонусных балла**, если **встроите эту функцию в программу из предыдущей домашки** и **сравните качества работы программы с нею и без неё**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Создание текстов.\n",
    "\n",
    "**Русский**: я написала сама небольшой текст, который состоит из разных штучек, которые могут быть сложными морфологическим парсерам: \n",
    "+ \"стали\" (гл vs. сущ.), \n",
    "+ \"мимо\" (нар. vs. предлог), \n",
    "+ \"благодаря\" (деепр. vs. предлог), \n",
    "+ \"сорок\" (числ. vs. сущ.), \n",
    "+ слова разных частей речи, чтобы посмотреть на работу в целом + всякие вещи, с которыми могут тоже возникнуть сложности при разметке (\"нью-йорк\", \"мисо-суп\", \"черно-белый\", \"из-за\", имя собственное - как поделит парсер?)\n",
    "\n",
    "**Английский:** тоже я сама написала небольшой текст **тоже с небольшими проверками на омонимичные формы слов**:\n",
    "+ \"mine\" (pron vs. noun vs. verb)\n",
    "+ \"use\" (noun vs. verb)\n",
    "+ \"whole\" (noun vs. adj.)\n",
    "+ \"while\" (noun vs. conj.)\n",
    "+ \"please\" (verb vs. adverb)\n",
    "+ разные части речи для проверки\n",
    "\n",
    "\n",
    "Теги я делала сама, старалась \"упрощать\" всякие местоимения-существительные/прилагательные в одну группу \"местоимений\", например, или причастие-деепричастие-инфинитив - это все глагол и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rus_list = ['Сегодня в Нью-Йорке хорошо, мы пошли в столовую и стали есть мисо-суп, салат и сорок пончиков.',\n",
    "                 'Благодаря официанту мы узнали, что ложки из стали такие же прочные, как и из серебра.',\n",
    "                 'Пять черно-белых сорок и желтых канареек за окном громко трещали.',\n",
    "                 'На улице светило солнце, проезжали мимо нас машины, люди на велосипедах, а мы шли и фотографировали понравившиеся здания и памятники.',\n",
    "                 'Ах, какая была отличная погода!',\n",
    "                 'Пройдя два квартала, мы почувствовали усталость из-за тяжелых сумок, давящих на плечи.',\n",
    "                 'Я захотела к себе домой, а моя подруга пригласила в свой дом на чашку кофе.',\n",
    "                 'Оля, моя подруга, сварила отличный кофе, но он мне не понравился.',\n",
    "                 'Потом мы быстро собрались и продолжили прогулку снова, благодаря солнце за погоду.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_sents = []\n",
    "for sent in text_rus_list:\n",
    "    rus_text_sents.append(' '.join([re.sub('[|\\.|\\?|!|,|\\(|\\)]','',w.lower()) for w in sent.split()]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_tags_list = ['ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB NOUN NOUN CONJ NUM NOUN',\n",
    "                      'PREP NOUN PRON VERB CONJ NOUN PREP NOUN PRON PART ADJ CONJ CONJ PREP NOUN',\n",
    "                      'NUM ADJ NOUN CONJ ADJ NOUN PREP NOUN ADV VERB',\n",
    "                      'PREP NOUN VERB NOUN VERB PREP PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN',\n",
    "                      'INTJ PRON VERB ADJ NOUN',\n",
    "                      'VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN VERB PREP NOUN',\n",
    "                      'PRON VERB PREP PRON ADV CONJ PRON NOUN VERB PREP PRON NOUN PREP NOUN NOUN',\n",
    "                      'NOUN PRON NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB',\n",
    "                      'ADV PRON ADV VERB CONJ VERB NOUN ADV VERB NOUN PREP NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_eng_list = ['I use my shorts, T-shirts and shirts for flirt.',\n",
    "                'The use of short sentences is popular nowadays.',\n",
    "                'Being rude is so rude.',\n",
    "                'Have you done your homework?',\n",
    "                 'I did it yesterday.',\n",
    "                'I have just done it.',\n",
    "                'I am doing it now.',\n",
    "                'Please, be mine!',\n",
    "                'I mine a whole on Fridays.',\n",
    "                'I saw a mine and I was scared.',\n",
    "                'Have you seen that mine shaft?',\n",
    "                'There is no other evidence of that in the whole world.',\n",
    "                'While you were doing your homework I made myself a monkey costume.',\n",
    "                'Please wait for a while!',\n",
    "                 'I want to please you',\n",
    "                'Have you seen three monkeys in the zoo?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text_sents = []\n",
    "for sent in text_eng_list:\n",
    "    eng_text_sents.append(' '.join([re.sub('[|\\.|\\?|!|,|\\(|\\)]','',w.lower()) for w in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text_tags_list = ['PRON VERB PRON NOUN NOUN CONJ NOUN ADP NOUN',\n",
    "                      'DET NOUN ADP ADJ NOUN VERB ADJ ADV',\n",
    "                      'VERB NOUN VERB ADV ADJ',\n",
    "                      'VERB PRON VERB PRON NOUN',\n",
    "                      'PRON VERB PRON NOUN',\n",
    "                      'PRON VERB ADV VERB PRON',\n",
    "                      'PRON VERB VERB PRON ADV',\n",
    "                      'ADV VERB PRON',\n",
    "                      'PRON VERB DET NOUN ADP NOUN',\n",
    "                      'PRON VERB DET NOUN CONJ NOUN VERB VERB',\n",
    "                      'VERB PRON VERB ADP NOUN NOUN',\n",
    "                      'DET VERB DET ADJ NOUN ADP DET ADP DET ADJ NOUN',\n",
    "                      'CONJ PRON VERB VERB PRON NOUN PRON VERB PRON DET NOUN NOUN',\n",
    "                      'ADV VERB ADP DET NOUN',\n",
    "                      'PRON VERB PRT VERB PRON',\n",
    "                      'VERB PRON VERB NUM NOUN ADP NUM NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_lists = {'rus_text_tags_list': rus_text_tags_list,'eng_text_tags_list':eng_text_tags_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Функции для оценки accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне проще сделать функцию здесь и \"по ходу пьесы\" смотреть accuracy каждого морфологического парсера)\n",
    "Я считаю максимальную схожесть, и не считаю проблемой, если какое-то слово поделено как-то не так или на два слова, я смотрю \"по максимуму\"\n",
    "В скобках приведена функция, в которой я доходила до места несоотвествия и \"впадала в ступор\", мне так не понравилось, поэтому я сделала по-другому:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def acc(x, lang):\n",
    "#     tags_list = tag_lists['{0}_text_tags_list'.format(lang)]\n",
    "#     ans = 0\n",
    "#     for sent_ind in range(len(tags_list)):\n",
    "#         sent = x[sent_ind]\n",
    "#         if len(sent.split()) == len(tags_list[sent_ind].split()):\n",
    "#             sent_acc = 0\n",
    "#             for i in range(len(sent.split())):\n",
    "#                 if sent.split()[i] == tags_list[sent_ind].split()[i]:\n",
    "#                     sent_acc += 1\n",
    "# #             print(sent_acc/len(sent.split()))\n",
    "#             ans += sent_acc/len(sent.split())\n",
    "#         else:\n",
    "#             if len(sent.split()) < len(tags_list[sent_ind].split()):\n",
    "#                 sent_acc = 0\n",
    "#                 for i in range(len(sent.split())):\n",
    "#                     if sent.split()[i] == tags_list[sent_ind].split()[i]:\n",
    "#                         sent_acc +=1\n",
    "#                     else:\n",
    "#                         print('1',sent_ind, sent, tags_list[sent_ind].split()[i:], sent.split()[i:])\n",
    "#             else:\n",
    "#                 sent_acc = 0\n",
    "#                 for i in range(len(tags_list[sent_ind].split())):\n",
    "#                     if sent.split()[i] == tags_list[sent_ind].split()[i]:\n",
    "#                         sent_acc +=1\n",
    "#                     else:\n",
    "#                         print('2', sent_ind, sent, tags_list[sent_ind].split()[i:], sent.split()[i:])\n",
    "#     return ans/len(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(x, lang):\n",
    "    tags_list = tag_lists['{0}_text_tags_list'.format(lang)]\n",
    "    ans = 0\n",
    "    for sent_ind in range(len(tags_list)):\n",
    "        A = x[sent_ind]\n",
    "        B = tags_list[sent_ind]\n",
    "        n = len(A)\n",
    "        m = len(B)\n",
    "        F = [[0] * (m + 1) for i in range(n + 1)]\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                if A[i - 1] == B[j - 1]:\n",
    "                    F[i][j] = F[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    F[i][j] = max(F[i - 1][j], F[i][j - 1])\n",
    "        Ans = []\n",
    "        i = n\n",
    "        j = m\n",
    "        while i > 0 and j > 0:\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                Ans.append(A[i - 1])\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif F[i - 1][j] == F[i][j]:\n",
    "                i -= 1\n",
    "            else:\n",
    "                j -= 1\n",
    "        Ans = Ans[::-1]\n",
    "        ans += len(''.join(Ans).split())/ len(x[sent_ind].split())\n",
    "    return ans / len(tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Морфологические парсеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_tags_dict = {'A': 'ADJ',\n",
    "                    'ADV': 'ADV',\n",
    "                    'ADVPRO': 'ADV',\n",
    "                    'ANUM': 'ADJ',\n",
    "                    'APRO': 'ADV',\n",
    "                    'COM': 'ADJ',\n",
    "                    'SCONJ': 'CONJ',\n",
    "                    'CONJ': 'CONJ',\n",
    "                    'INTJ': 'INTJ',\n",
    "                    'NONLEX': 'X',\n",
    "                    'NUM': 'NUM',\n",
    "                    'PART': 'PART',\n",
    "                    'PR': 'PREP',\n",
    "                    'S': 'NOUN',\n",
    "                    'SPRO': 'PRON',\n",
    "                    'UNKN': 'X',\n",
    "                    'V': 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_tags = []\n",
    "for sent in rus_text_sents:\n",
    "    ans = ''\n",
    "    for a in mystem_analyzer.analyze(sent):\n",
    "        if 'analysis' in a:\n",
    "            if 'gr' in a['analysis'][0].keys():\n",
    "                gr = a['analysis'][0]['gr']\n",
    "                pos = gr.split('=')[0].split(',')[0]\n",
    "                ans += mystem_tags_dict[pos] + ' '\n",
    "    mystem_tags.append(ans[:-1])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  сегодня в нью-йорке хорошо мы пошли в столовую и стали есть мисо-суп салат и сорок пончиков \n",
      "     ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB NOUN NOUN NOUN CONJ NUM NOUN \n",
      "     ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB NOUN NOUN CONJ NUM NOUN\n",
      "1 )  благодаря официанту мы узнали что ложки из стали такие же прочные как и из серебра \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP VERB ADV PART ADJ CONJ CONJ PREP NOUN \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP NOUN PRON PART ADJ CONJ CONJ PREP NOUN\n",
      "2 )  пять черно-белых сорок и желтых канареек за окном громко трещали \n",
      "     NUM ADJ NUM CONJ ADJ NOUN PREP NOUN ADV VERB \n",
      "     NUM ADJ NOUN CONJ ADJ NOUN PREP NOUN ADV VERB\n",
      "3 )  на улице светило солнце проезжали мимо нас машины люди на велосипедах а мы шли и фотографировали понравившиеся здания и памятники \n",
      "     PREP NOUN VERB NOUN VERB PREP PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN \n",
      "     PREP NOUN VERB NOUN VERB PREP PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN\n",
      "4 )  ах какая была отличная погода \n",
      "     INTJ ADV VERB ADJ NOUN \n",
      "     INTJ PRON VERB ADJ NOUN\n",
      "5 )  пройдя два квартала мы почувствовали усталость из-за тяжелых сумок давящих на плечи \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN ADJ PREP NOUN \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN VERB PREP NOUN\n",
      "6 )  я захотела к себе домой а моя подруга пригласила в свой дом на чашку кофе \n",
      "     PRON VERB PREP PRON ADV CONJ ADV NOUN VERB PREP ADV NOUN PREP NOUN NOUN \n",
      "     PRON VERB PREP PRON ADV CONJ PRON NOUN VERB PREP PRON NOUN PREP NOUN NOUN\n",
      "7 )  оля моя подруга сварила отличный кофе но он мне не понравился \n",
      "     NOUN ADV NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB \n",
      "     NOUN PRON NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB\n",
      "8 )  потом мы быстро собрались и продолжили прогулку снова благодаря солнце за погоду \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV PREP NOUN PREP NOUN \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV VERB NOUN PREP NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(mystem_tags)):\n",
    "    print(s, ') ', rus_text_sents[s], '\\n', '   ', mystem_tags[s],'\\n', '   ', rus_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929659338482868"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(mystem_tags, 'rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) PYMORPHY2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_tags_dict = {\n",
    "    'NOUN': 'NOUN',\n",
    "    'VERB': 'VERB',\n",
    "    'PRTF': 'VERB',\n",
    "    'GRND': 'VERB',\n",
    "    'ADJF': 'ADJ',\n",
    "    'ADJS': 'ADJ',\n",
    "    'PREP': 'PREP',\n",
    "    'SPRO': 'PRON',\n",
    "    'APRO': 'PRON',\n",
    "    'NPRO': 'PRON',\n",
    "    'ADVPRO': 'PRON',\n",
    "    'ADVB': 'ADV',\n",
    "    'CONJ': 'CONJ',\n",
    "    'NUMR': 'NUM',\n",
    "    'PRCL': 'PART',\n",
    "    'INTJ': 'INTJ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_tags = []\n",
    "for sent in rus_text_sents:\n",
    "    ans = ''\n",
    "    for word in sent.split():\n",
    "        p = str(m.parse(word)[0].tag)\n",
    "        ans += pymorphy_tags_dict[re.findall('[A-Z]{1,5}', p)[0]] + ' '\n",
    "    pymorphy_tags.append(ans[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  сегодня в нью-йорке хорошо мы пошли в столовую и стали есть мисо-суп салат и сорок пончиков \n",
      "     ADV PREP NOUN ADV PRON VERB PREP ADJ CONJ VERB INTJ NOUN NOUN CONJ NUM NOUN \n",
      "     ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB NOUN NOUN CONJ NUM NOUN\n",
      "1 )  благодаря официанту мы узнали что ложки из стали такие же прочные как и из серебра \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP VERB ADJ PART ADJ CONJ CONJ PREP NOUN \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP NOUN PRON PART ADJ CONJ CONJ PREP NOUN\n",
      "2 )  пять черно-белых сорок и желтых канареек за окном громко трещали \n",
      "     NUM ADJ NUM CONJ ADJ NOUN PREP NOUN ADV VERB \n",
      "     NUM ADJ NOUN CONJ ADJ NOUN PREP NOUN ADV VERB\n",
      "3 )  на улице светило солнце проезжали мимо нас машины люди на велосипедах а мы шли и фотографировали понравившиеся здания и памятники \n",
      "     PREP NOUN VERB NOUN VERB ADV PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN \n",
      "     PREP NOUN VERB NOUN VERB PREP PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN\n",
      "4 )  ах какая была отличная погода \n",
      "     INTJ ADJ VERB ADJ NOUN \n",
      "     INTJ PRON VERB ADJ NOUN\n",
      "5 )  пройдя два квартала мы почувствовали усталость из-за тяжелых сумок давящих на плечи \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN ADJ PREP NOUN \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN VERB PREP NOUN\n",
      "6 )  я захотела к себе домой а моя подруга пригласила в свой дом на чашку кофе \n",
      "     PRON VERB PREP PRON ADV CONJ ADJ NOUN VERB PREP ADJ NOUN PREP NOUN NOUN \n",
      "     PRON VERB PREP PRON ADV CONJ PRON NOUN VERB PREP PRON NOUN PREP NOUN NOUN\n",
      "7 )  оля моя подруга сварила отличный кофе но он мне не понравился \n",
      "     NOUN ADJ NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB \n",
      "     NOUN PRON NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB\n",
      "8 )  потом мы быстро собрались и продолжили прогулку снова благодаря солнце за погоду \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV PREP NOUN PREP NOUN \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV VERB NOUN PREP NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(pymorphy_tags)):\n",
    "    print(s, ') ', rus_text_sents[s], '\\n', '   ', pymorphy_tags[s],'\\n', '   ', rus_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9167508417508416"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(pymorphy_tags, 'rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) NATASHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_tags_dict = {\n",
    "    'NOUN': 'NOUN',\n",
    "    'VERB': 'VERB',\n",
    "    'AUX': 'VERB',\n",
    "    'ADJ': 'ADJ',\n",
    "    'PREP': 'PREP',\n",
    "    'PRON': 'PRON',\n",
    "    'DET': 'PRON',\n",
    "    'ADV': 'ADV',\n",
    "    'CCONJ': 'CONJ',\n",
    "    'SCONJ': 'CONJ',\n",
    "    'NUM': 'NUM',\n",
    "    'PART': 'PART',\n",
    "    'ADP':'PREP'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natasha\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_vocab = MorphVocab()\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_tags = []\n",
    "for sent in rus_text_sents:\n",
    "    doc = Doc(sent)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    natasha_tags.append(' '.join([ natasha_tags_dict[_.pos] for _ in doc.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  сегодня в нью-йорке хорошо мы пошли в столовую и стали есть мисо-суп салат и сорок пончиков \n",
      "     ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB ADJ NOUN CONJ NUM NOUN \n",
      "     ADV PREP NOUN ADV PRON VERB PREP NOUN CONJ VERB VERB NOUN NOUN CONJ NUM NOUN\n",
      "1 )  благодаря официанту мы узнали что ложки из стали такие же прочные как и из серебра \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP VERB PRON PART ADJ CONJ PART PREP NOUN \n",
      "     PREP NOUN PRON VERB CONJ NOUN PREP NOUN PRON PART ADJ CONJ CONJ PREP NOUN\n",
      "2 )  пять черно-белых сорок и желтых канареек за окном громко трещали \n",
      "     NUM ADJ NOUN CONJ ADJ NOUN PREP NOUN ADV VERB \n",
      "     NUM ADJ NOUN CONJ ADJ NOUN PREP NOUN ADV VERB\n",
      "3 )  на улице светило солнце проезжали мимо нас машины люди на велосипедах а мы шли и фотографировали понравившиеся здания и памятники \n",
      "     PREP NOUN VERB NOUN VERB ADV PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB ADJ NOUN CONJ NOUN \n",
      "     PREP NOUN VERB NOUN VERB PREP PRON NOUN NOUN PREP NOUN CONJ PRON VERB CONJ VERB VERB NOUN CONJ NOUN\n",
      "4 )  ах какая была отличная погода \n",
      "     NOUN PRON VERB ADJ NOUN \n",
      "     INTJ PRON VERB ADJ NOUN\n",
      "5 )  пройдя два квартала мы почувствовали усталость из-за тяжелых сумок давящих на плечи \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN NOUN PREP NOUN \n",
      "     VERB NUM NOUN PRON VERB NOUN PREP ADJ NOUN VERB PREP NOUN\n",
      "6 )  я захотела к себе домой а моя подруга пригласила в свой дом на чашку кофе \n",
      "     PRON VERB PREP PRON ADV CONJ PRON NOUN VERB PREP PRON NOUN PREP NOUN NOUN \n",
      "     PRON VERB PREP PRON ADV CONJ PRON NOUN VERB PREP PRON NOUN PREP NOUN NOUN\n",
      "7 )  оля моя подруга сварила отличный кофе но он мне не понравился \n",
      "     ADJ PRON NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB \n",
      "     NOUN PRON NOUN VERB ADJ NOUN CONJ PRON PRON PART VERB\n",
      "8 )  потом мы быстро собрались и продолжили прогулку снова благодаря солнце за погоду \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV PREP NOUN PREP NOUN \n",
      "     ADV PRON ADV VERB CONJ VERB NOUN ADV VERB NOUN PREP NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(natasha_tags)):\n",
    "    print(s, ') ', rus_text_sents[s], '\\n', '   ', natasha_tags[s],'\\n', '   ', rus_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9477693602693604"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(natasha_tags, 'rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tags_dict = {\n",
    "    'NOUN': 'NOUN',\n",
    "    'VERB': 'VERB',\n",
    "    'AUX': 'VERB',\n",
    "    'ADJ': 'ADJ',\n",
    "    'PREP': 'PREP',\n",
    "    'PRON': 'PRON',\n",
    "    'DET': 'DET',\n",
    "    'ADV': 'ADV',\n",
    "    'CCONJ': 'CONJ',\n",
    "    'SCONJ': 'CONJ',\n",
    "    'NUM': 'NUM',\n",
    "    'PART': 'PRT',\n",
    "    'ADP':'VERB',\n",
    "    'PUNCT':'',\n",
    "    'INTJ': 'ADV'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tags = []\n",
    "for sent in eng_text_sents:\n",
    "    for s in nlp(sent).sents:\n",
    "        ans = ''\n",
    "        for t in s:\n",
    "            ans += spacy_tags_dict[t.pos_] + ' '\n",
    "        spacy_tags.append(ans[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  i use my shorts t-shirts and shirts for flirt \n",
      "     PRON VERB DET NOUN NOUN  NOUN CONJ NOUN VERB NOUN \n",
      "     PRON VERB PRON NOUN NOUN CONJ NOUN ADP NOUN\n",
      "1 )  the use of short sentences is popular nowadays \n",
      "     DET NOUN VERB ADJ NOUN VERB ADJ ADV \n",
      "     DET NOUN ADP ADJ NOUN VERB ADJ ADV\n",
      "2 )  being rude is so rude \n",
      "     VERB ADJ VERB ADV ADJ \n",
      "     VERB NOUN VERB ADV ADJ\n",
      "3 )  have you done your homework \n",
      "     VERB PRON VERB DET NOUN \n",
      "     VERB PRON VERB PRON NOUN\n",
      "4 )  i did it yesterday \n",
      "     PRON VERB PRON NOUN \n",
      "     PRON VERB PRON NOUN\n",
      "5 )  i have just done it \n",
      "     PRON VERB ADV VERB PRON \n",
      "     PRON VERB ADV VERB PRON\n",
      "6 )  i am doing it now \n",
      "     PRON VERB VERB PRON ADV \n",
      "     PRON VERB VERB PRON ADV\n",
      "7 )  please be mine \n",
      "     ADV VERB PRON \n",
      "     ADV VERB PRON\n",
      "8 )  i mine a whole on fridays \n",
      "     PRON VERB DET NOUN VERB NOUN \n",
      "     PRON VERB DET NOUN ADP NOUN\n",
      "9 )  i saw a mine and i was scared \n",
      "     PRON VERB DET NOUN CONJ PRON VERB ADJ \n",
      "     PRON VERB DET NOUN CONJ NOUN VERB VERB\n",
      "10 )  have you seen that mine shaft \n",
      "     VERB PRON VERB DET NOUN NOUN \n",
      "     VERB PRON VERB ADP NOUN NOUN\n",
      "11 )  there is no other evidence of that in the whole world \n",
      "     PRON VERB DET ADJ NOUN VERB DET VERB DET ADJ NOUN \n",
      "     DET VERB DET ADJ NOUN ADP DET ADP DET ADJ NOUN\n",
      "12 )  while you were doing your homework i made myself a monkey costume \n",
      "     CONJ PRON VERB VERB DET NOUN PRON VERB PRON DET NOUN NOUN \n",
      "     CONJ PRON VERB VERB PRON NOUN PRON VERB PRON DET NOUN NOUN\n",
      "13 )  please wait for a while \n",
      "     ADV VERB VERB DET NOUN \n",
      "     ADV VERB ADP DET NOUN\n",
      "14 )  i want to please you \n",
      "     PRON VERB PRT VERB PRON \n",
      "     PRON VERB TO VERB PRON\n",
      "15 )  have you seen three monkeys in the zoo \n",
      "     VERB PRON VERB NUM NOUN VERB DET NOUN \n",
      "     VERB PRON VERB NUM NOUN ADP NUM NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(spacy_tags)):\n",
    "    print(s, ') ', eng_text_sents[s], '\\n', '   ', spacy_tags[s],'\\n', '   ', eng_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860795454545455\n"
     ]
    }
   ],
   "source": [
    "print(acc(spacy_tags,'eng'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_tags_dict = {\n",
    "    'NNS': 'NOUN',\n",
    "    'NN': 'NOUN',\n",
    "    'VB': 'VERB',\n",
    "    'VBP': 'VERB',\n",
    "    'VBZ': 'VERB',\n",
    "    'VBN': 'VERB',\n",
    "    'VBD': 'VERB',\n",
    "    'VBG': 'VERB',\n",
    "    'DT': 'DET',\n",
    "    'EX': 'DET',\n",
    "    'JJ': 'ADJ',\n",
    "    'IN': 'CONJ',\n",
    "    'PRP': 'PRON',\n",
    "    'PRP$': 'PRON',\n",
    "    'RB': 'ADV',\n",
    "    'UH': 'ADV',\n",
    "    'CC': 'CONJ',\n",
    "    'SCONJ': 'CONJ',\n",
    "    'CD': 'NUM',\n",
    "    'PART': 'PART',\n",
    "    'INN':'ADP',\n",
    "    'TO': 'PRT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_tags = []\n",
    "for sent in eng_text_sents:\n",
    "    sentence = Sentence(sent)\n",
    "    tagger.predict(sentence)\n",
    "    ans = ''\n",
    "    for tags in re.findall('<(.*?)>',sentence.to_tagged_string()):\n",
    "        ans += flair_tags_dict[tags] + ' '\n",
    "    flair_tags.append(ans[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  i use my shorts t-shirts and shirts for flirt \n",
      "     PRON VERB PRON NOUN NOUN CONJ NOUN CONJ NOUN \n",
      "     PRON VERB PRON NOUN NOUN CONJ NOUN ADP NOUN\n",
      "1 )  the use of short sentences is popular nowadays \n",
      "     DET NOUN CONJ ADJ NOUN VERB ADJ ADV \n",
      "     DET NOUN ADP ADJ NOUN VERB ADJ ADV\n",
      "2 )  being rude is so rude \n",
      "     VERB ADJ VERB ADV ADJ \n",
      "     VERB NOUN VERB ADV ADJ\n",
      "3 )  have you done your homework \n",
      "     VERB PRON VERB PRON NOUN \n",
      "     VERB PRON VERB PRON NOUN\n",
      "4 )  i did it yesterday \n",
      "     PRON VERB PRON NOUN \n",
      "     PRON VERB PRON NOUN\n",
      "5 )  i have just done it \n",
      "     PRON VERB ADV VERB PRON \n",
      "     PRON VERB ADV VERB PRON\n",
      "6 )  i am doing it now \n",
      "     PRON VERB VERB PRON ADV \n",
      "     PRON VERB VERB PRON ADV\n",
      "7 )  please be mine \n",
      "     ADV VERB PRON \n",
      "     ADV VERB PRON\n",
      "8 )  i mine a whole on fridays \n",
      "     PRON VERB DET NOUN CONJ NOUN \n",
      "     PRON VERB DET NOUN ADP NOUN\n",
      "9 )  i saw a mine and i was scared \n",
      "     PRON VERB DET NOUN CONJ PRON VERB ADJ \n",
      "     PRON VERB DET NOUN CONJ NOUN VERB VERB\n",
      "10 )  have you seen that mine shaft \n",
      "     VERB PRON VERB DET NOUN NOUN \n",
      "     VERB PRON VERB ADP NOUN NOUN\n",
      "11 )  there is no other evidence of that in the whole world \n",
      "     DET VERB DET ADJ NOUN CONJ DET CONJ DET ADJ NOUN \n",
      "     DET VERB DET ADJ NOUN ADP DET ADP DET ADJ NOUN\n",
      "12 )  while you were doing your homework i made myself a monkey costume \n",
      "     CONJ PRON VERB VERB PRON NOUN PRON VERB PRON DET NOUN NOUN \n",
      "     CONJ PRON VERB VERB PRON NOUN PRON VERB PRON DET NOUN NOUN\n",
      "13 )  please wait for a while \n",
      "     ADV VERB CONJ DET NOUN \n",
      "     ADV VERB ADP DET NOUN\n",
      "14 )  i want to please you \n",
      "     PRON VERB PRT VERB PRON \n",
      "     PRON VERB PRT VERB PRON\n",
      "15 )  have you seen three monkeys in the zoo \n",
      "     VERB PRON VERB NUM NOUN CONJ DET NOUN \n",
      "     VERB PRON VERB NUM NOUN ADP NUM NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(flair_tags)):\n",
    "    print(s, ') ', eng_text_sents[s], '\\n', '   ', flair_tags[s],'\\n', '   ', eng_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9228377525252526\n"
     ]
    }
   ],
   "source": [
    "print(acc(flair_tags, 'eng'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tags = []\n",
    "for i in range(len(eng_text_sents)):\n",
    "    tokens=nltk.word_tokenize(eng_text_sents[i])\n",
    "    ans = ''\n",
    "    for tup in nltk.pos_tag(tokens, tagset='universal'):\n",
    "        ans += tup[1] + ' '\n",
    "    nltk_tags.append(ans[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 )  i use my shorts t-shirts and shirts for flirt \n",
      "     NOUN VERB PRON NOUN NOUN CONJ NOUN ADP NOUN \n",
      "     PRON VERB PRON NOUN NOUN CONJ NOUN ADP NOUN\n",
      "1 )  the use of short sentences is popular nowadays \n",
      "     DET NOUN ADP ADJ NOUN VERB ADJ NOUN \n",
      "     DET NOUN ADP ADJ NOUN VERB ADJ ADV\n",
      "2 )  being rude is so rude \n",
      "     VERB NOUN VERB ADV ADJ \n",
      "     VERB NOUN VERB ADV ADJ\n",
      "3 )  have you done your homework \n",
      "     VERB PRON VERB PRON NOUN \n",
      "     VERB PRON VERB PRON NOUN\n",
      "4 )  i did it yesterday \n",
      "     NOUN VERB PRON NOUN \n",
      "     PRON VERB PRON NOUN\n",
      "5 )  i have just done it \n",
      "     NOUN VERB ADV VERB PRON \n",
      "     PRON VERB ADV VERB PRON\n",
      "6 )  i am doing it now \n",
      "     NOUN VERB VERB PRON ADV \n",
      "     PRON VERB VERB PRON ADV\n",
      "7 )  please be mine \n",
      "     NOUN VERB ADJ \n",
      "     ADV VERB PRON\n",
      "8 )  i mine a whole on fridays \n",
      "     NOUN VERB DET NOUN ADP NOUN \n",
      "     PRON VERB DET NOUN ADP NOUN\n",
      "9 )  i saw a mine and i was scared \n",
      "     NOUN VERB DET NOUN CONJ NOUN VERB VERB \n",
      "     PRON VERB DET NOUN CONJ NOUN VERB VERB\n",
      "10 )  have you seen that mine shaft \n",
      "     VERB PRON VERB ADP NOUN NOUN \n",
      "     VERB PRON VERB ADP NOUN NOUN\n",
      "11 )  there is no other evidence of that in the whole world \n",
      "     DET VERB DET ADJ NOUN ADP DET ADP DET ADJ NOUN \n",
      "     DET VERB DET ADJ NOUN ADP DET ADP DET ADJ NOUN\n",
      "12 )  while you were doing your homework i made myself a monkey costume \n",
      "     ADP PRON VERB VERB PRON NOUN NOUN VERB PRON DET NOUN NOUN \n",
      "     CONJ PRON VERB VERB PRON NOUN PRON VERB PRON DET NOUN NOUN\n",
      "13 )  please wait for a while \n",
      "     VERB NOUN ADP DET NOUN \n",
      "     ADV VERB ADP DET NOUN\n",
      "14 )  i want to please you \n",
      "     NOUN VERB PRT VERB PRON \n",
      "     PRON VERB TO VERB PRON\n",
      "15 )  have you seen three monkeys in the zoo \n",
      "     VERB PRON VERB NUM NOUN ADP DET NOUN \n",
      "     VERB PRON VERB NUM NOUN ADP NUM NOUN\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(nltk_tags)):\n",
    "    print(s, ') ', eng_text_sents[s], '\\n', '   ', nltk_tags[s],'\\n', '   ', eng_text_tags_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9249999999999999\n"
     ]
    }
   ],
   "source": [
    "print(acc(nltk_tags, 'eng'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Лучший теггер + прошлая домашка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как прекрасно сохранять все в базу данных, открываем бд с прошлой домашки, пишем одну функцию и сдаем домашку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('NLP-HW1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя идея очень простая: с помощью лучшего теггера (им оказался natasha) я найду синтаксические группы (ниже описание и причина выбора), которые просто доставлю в конец лемм, чтобы их как бы сделать \"тяжелее\" как параметр позитивного/негативного отзыва. Мне кажется это логичным, тк мы типа на момент этой домашки ничего особо не знали про tf-idf, а идея с \"большим весом\" логичная - что чаще встречается, что синтаксически важнее, то и больше учитываем при выборе сентимента\n",
    "Как синтаксически группы?\n",
    "+ \"**существительное + прилагательное**\" - потому что часто в отзывах мы смотрим на характеристики чего-либо (например, \"Это **отличное заведение**!\" или \"**Ужасный сервис** и **холодные блюда**\" - и сразу все понятно\n",
    "+ \"не\" + глагол - здесь тоже понятно, хочется думать про всякие вещи типа \"Мне **не понравилось/ не хочется** приходить еще\" и тд\n",
    "+ \"**существительное + существительное**\" - тут хочется искать всякие \"выбор игрушек\", \"поведение официанта\", чтобы понимать фокус отзыва - как вообще человек оценивал заведение/место\n",
    "\n",
    "Я не гарантирую, что все это сработает или идеально будет находить такое, но можно попробовать:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NewsSyntaxParser\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "\n",
    "\n",
    "def syntax(sent):\n",
    "    ans = []\n",
    "    adj_noun_list = []\n",
    "    np_list = []\n",
    "    no_verb_list = []\n",
    "    doc = Doc(sent)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    for i in range(len(doc.tokens)):\n",
    "        np = ''\n",
    "        if list(doc.tokens[i])[6] == 'ADJ':\n",
    "            head = int(list(doc.tokens[i])[4].split('_')[-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'NOUN':\n",
    "                adj_noun_list.append(list(doc.tokens[i])[2] + ' ' + list(doc.tokens[head])[2])\n",
    "        elif list(doc.tokens[i])[2] == 'не':\n",
    "            head = int(list(doc.tokens[i])[4].split('_')[-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'VERB':\n",
    "                no_verb_list.append('не ' + list(doc.tokens[head])[2])\n",
    "        elif list(doc.tokens[i])[6] == 'NOUN':\n",
    "            head = int(list(doc.tokens[i])[4][-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'NOUN':\n",
    "                np_list.append(list(doc.tokens[i])[2] + ' ' + list(doc.tokens[head])[2])\n",
    "    ans.extend(adj_noun_list)\n",
    "    ans.extend(np_list)\n",
    "    ans.extend(no_verb_list)\n",
    "    return sent + ' ' + ' '.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['l_s'] = df['lemmas'].apply(lambda x: syntax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделали, теперь хорошо бы импортировать функции из прошлой домашки, но, честно говоря, мне проще руками скопировать несложный код, хотя я понимаю, что так не очыень круто "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy на тестовой выборке:  0.55\n"
     ]
    }
   ],
   "source": [
    "#тестовая выборка\n",
    "pos_rev_lem_list = df[(df['rates']==5)]['l_s'].to_list()[:-10]\n",
    "neg_rev_lem_list = df[(df['rates']==1)]['l_s'].to_list()[:-10]\n",
    "\n",
    "#словари со словами, встречающихся в только положительных или только отрицательных отзывах\n",
    "to_string_pos = ''\n",
    "for el in pos_rev_lem_list:\n",
    "    to_string_pos += ' ' + el\n",
    "to_string_neg = ''\n",
    "for el in neg_rev_lem_list:\n",
    "    to_string_neg += ' ' + el\n",
    "    \n",
    "from collections import Counter\n",
    "pos_rev_c = Counter(to_string_pos.split())\n",
    "neg_rev_c = Counter(to_string_neg.split())\n",
    "\n",
    "pos_rev_set = set([tup[0] for tup in pos_rev_c.most_common() if 25 > tup[1] > 1 and tup[0].isalpha()])\n",
    "neg_rev_set = set([tup[0] for tup in neg_rev_c.most_common() if 50 > tup[1] > 1 and tup[0].isalpha()])\n",
    "\n",
    "only_pos = pos_rev_set - pos_rev_set.intersection(neg_rev_set)\n",
    "only_neg = neg_rev_set - pos_rev_set.intersection(neg_rev_set)\n",
    "\n",
    "#функция для определения сентимента\n",
    "def which_sent(x):\n",
    "    pos_sent = 0\n",
    "    neg_sent = 0\n",
    "    for word in x.split():\n",
    "        if word in only_pos:\n",
    "            pos_sent +=1\n",
    "        elif word in only_neg:\n",
    "            neg_sent +=1\n",
    "        else:\n",
    "            pass\n",
    "    if pos_sent > neg_sent:\n",
    "        return 5\n",
    "    else:\n",
    "        return 1\n",
    "#Тестовая выборка\n",
    "X_test = df[(df['rates']==5)]['l_s'].to_list()[-10:] + df[(df['rates']==1)]['l_s'].to_list()[-10:]\n",
    "y_test = df[(df['rates']==5)]['rates'].to_list()[-10:] + df[(df['rates']==1)]['rates'].to_list()[-10:]\n",
    "\n",
    "#подсчет accuracy\n",
    "def count_accuracy(X, y):\n",
    "    ans = 0\n",
    "    for i in range(len(X)):\n",
    "        if which_sent(X[i]) == y[i]:\n",
    "            ans += 1\n",
    "    return ans/len(y)\n",
    "\n",
    "print('accuracy на тестовой выборке: ', count_accuracy(X_test, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ого, отлично, стало лучше (по сравнению с прошлым разом)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но все еще некруто, хочу отдельно взять те найденные синтаксические группы и добавить их отдельно в словарь!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntax2(sent):\n",
    "    ans = []\n",
    "    adj_noun_list = []\n",
    "    np_list = []\n",
    "    no_verb_list = []\n",
    "    doc = Doc(sent)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    for i in range(len(doc.tokens)):\n",
    "        np = ''\n",
    "        if list(doc.tokens[i])[6] == 'ADJ':\n",
    "            head = int(list(doc.tokens[i])[4].split('_')[-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'NOUN':\n",
    "                adj_noun_list.append(list(doc.tokens[i])[2] + ' ' + list(doc.tokens[head])[2])\n",
    "        elif list(doc.tokens[i])[2] == 'не':\n",
    "            head = int(list(doc.tokens[i])[4].split('_')[-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'VERB':\n",
    "                no_verb_list.append('не ' + list(doc.tokens[head])[2])\n",
    "        elif list(doc.tokens[i])[6] == 'NOUN':\n",
    "            head = int(list(doc.tokens[i])[4][-1]) - 1\n",
    "            if list(doc.tokens[head])[6] == 'NOUN':\n",
    "                np_list.append(list(doc.tokens[i])[2] + ' ' + list(doc.tokens[head])[2])\n",
    "    ans.extend(adj_noun_list)\n",
    "    ans.extend(np_list)\n",
    "    ans.extend(no_verb_list)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev_l = []\n",
    "neg_rev_l = []\n",
    "for i in range(len(df['lemmas'].to_list())):\n",
    "    if df['rates'].to_list()[i] == 5:\n",
    "        pos_rev_l.extend(syntax2(df['lemmas'].to_list()[i]))\n",
    "    else:\n",
    "        neg_rev_l.extend(syntax2(df['lemmas'].to_list()[i]))\n",
    "\n",
    "pos_rev_set = pos_rev_set.union(set(pos_rev_l))\n",
    "neg_rev_set = neg_rev_set.union(set(neg_rev_l))\n",
    "\n",
    "# for i in range(len(df['lemmas'].to_list())):\n",
    "#     if df['rates'].to_list()[i] == 5:\n",
    "#         pos_rev_set.union(syntax2(df['lemmas'].to_list()[i]))\n",
    "#     else:\n",
    "#         neg_rev_set.union(syntax2(df['lemmas'].to_list()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy на тестовой выборке:  0.9\n"
     ]
    }
   ],
   "source": [
    "only_pos = pos_rev_set - pos_rev_set.intersection(neg_rev_set)\n",
    "only_neg = neg_rev_set - pos_rev_set.intersection(neg_rev_set)\n",
    "\n",
    "#функция для определения сентимента\n",
    "def which_sent2(x):\n",
    "    pos_sent = 0\n",
    "    neg_sent = 0\n",
    "    l = x.split()\n",
    "    for i in range(len(l)-1):\n",
    "        if l[i] in only_pos:\n",
    "            pos_sent +=1\n",
    "        elif l[i] in only_neg:\n",
    "            neg_sent +=1\n",
    "        elif l[i] + ' ' + l[i+1] in only_pos:\n",
    "            pos_sent +=1 \n",
    "        elif l[i] + ' ' + l[i+1] in only_neg:\n",
    "            neg_sent +=1    \n",
    "    if pos_sent > neg_sent:\n",
    "        return 5\n",
    "    else:\n",
    "        return 1\n",
    "                   \n",
    "                \n",
    "#Тестовая выборка\n",
    "X_test = df[(df['rates']==5)]['lemmas'].to_list()[-10:] + df[(df['rates']==1)]['lemmas'].to_list()[-10:]\n",
    "y_test = df[(df['rates']==5)]['rates'].to_list()[-10:] + df[(df['rates']==1)]['rates'].to_list()[-10:]\n",
    "\n",
    "\n",
    "#подсчет accuracy\n",
    "def count_accuracy(X, y):\n",
    "    ans = 0\n",
    "    for i in range(len(X)):\n",
    "        if which_sent2(X[i]) == y[i]:\n",
    "            ans += 1\n",
    "    return ans/len(y)\n",
    "\n",
    "print('accuracy на тестовой выборке: ', count_accuracy(X_test, y_test) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
