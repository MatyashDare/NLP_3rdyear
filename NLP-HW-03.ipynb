{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоматическая обработка языка\n",
    "## НИУ ВШЭ, 2020-2021 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил(а): Дарья Матяш\n",
    "\n",
    "Ссылка на условия и требования: https://github.com/named-entity/hse-nlp/blob/master/DZ3_mod.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой домашней работе вам предстоит составить csv или pandas таблицу, в которой будет хранится информация о широком или узком топике текста.\n",
    "Мы будем работать с тем же датасетом, что и на последнем семинаре по gensim.\n",
    "Для начала определим широкий топик текста с помощью gensim. Так, На семинаре мы познакомились с \"обычным\" встроенным дженсимом, но у него есть улучшенная версия - mallet, для запуска которой надо скачать zip file, распаковать его, дать обертке для маллета path к файлу.... и просто запустить.\n",
    "\n",
    "За **\"прогон\" дженсима** в его базовой версии вы получите - один балл, за прогон **с маллетом** - **2** (1 основной и один бонусный).\n",
    "**Создайте функцию или серию функций**, через которую будет удобно **подобрать оптимальное число групп** - **1 балл** за нахождение оптимального числа групп, **1 балл** - если этобудет не руками, а через **функцию** **2 балла**\n",
    "\n",
    "\n",
    "Как вы знаете, gensim считает, что каждый текст содержит несколько топиков, но на следующем этапе вам будет надо создать функцию, которая будет для каждого текста определять один широкий топик, самый главный.\n",
    "Мы предлагаем сделать это так: создайте счётчик, и каждый раз, когда в тексте будет встречаться одно из слов, соответсвующих данной теме, добавляйте к счетчику его вес. **2 балла**\n",
    "\n",
    "\n",
    "После того, как у вас получится какое-то количество групп (наборов текстов с общим топиком). Внутри каждой из этих групп посчитайте **тф_идф для каждого текста** (Т. е. возьмите все тексты с одинаковой темой за ваш корпус и посчитайте для каждого из этих текстов тф_идф). **3 балла**\n",
    "Каждому тексту определите **слова с пятью самыми высокими тф_идф** и запишите их в таблицу\n",
    "вывод - excel, csv или pandas таблица с текстом, его широким топиком по дженсиму и 5 тф_идф словами для этого текста - **1 балл**\n",
    "Логичность и красота кода - **1 балл**\n",
    "ещё один бонус балл: описать как работает coherence score (не успели обсудить на паре) на русском - словами\n",
    "дэдлайн 19 окт 23:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальное число топиков в датасете: 20\n"
     ]
    }
   ],
   "source": [
    "print('Изначальное число топиков в датасете:', len(df.target_names.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tokens = list(sent_to_words(df['content'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thing', 'car', 'nntp', 'posting', 'host', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(content_tokens)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_lemmatized'] = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем, что изначальное число топиков в датасете - 20, поищем оптимальное автоматические разбиение на топики от 1 до 30 возможных топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'path/to/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_scrores = []\n",
    "for i in range(1,30):\n",
    "    mallet_path = '/Users/pikachu/Downloads/mallet-2.0.8/bin/mallet'\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word)\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_scrores.append(coherence_model_ldamallet.get_coherence())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оптимальное число топиков:  22\n"
     ]
    }
   ],
   "source": [
    "print('оптимальное число топиков: ', coherence_scrores.index(max(coherence_scrores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = coherence_scrores.index(max(coherence_scrores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие топики с какими словами выделены (по умолчанию на каждый топик выводится по 10 слов - мне кажется, это отлично"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = {}\n",
    "topics_weights = {}\n",
    "for i in range(len(ldamallet.show_topics(formatted=False, num_topics=num_topics))):\n",
    "    topics_dict[i] = list([tup[0] for tup in ldamallet.show_topics(formatted=False, num_topics=num_topics)[i][1]])\n",
    "    topics_weights[i] = list([tup[1] for tup in ldamallet.show_topics(formatted=False, num_topics=num_topics)[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем для каждого текста его топик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(x):\n",
    "    ans_dict = {i: 0 for i in range(22)}\n",
    "    for word in x:\n",
    "        for i in range(num_topics):\n",
    "            if word in topics_dict[i]:\n",
    "                ans_dict[i] += topics_weights[i][topics_dict[i].index(word)]\n",
    "    return max(ans_dict, key=ans_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_topic'] = df['data_lemmatized'].apply(predict_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>data_lemmatized</th>\n",
       "      <th>predicted_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[thing, car, nntp, posting, host, line, wonder...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>[final, summary, final, call, si, clock, repor...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>[question, computer, network, distribution, li...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[division, line, distribution, world, nntp, po...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>[article, owcb, world, std, com, tombaker, wor...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "      <td>[factory, scan, distribution, factory, line, c...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>[problem, screen, blank, sometimes, minor, phy...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>[este, mount, case, line, instal, try, mount, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>[line, post, host, write, boy, embarasse, triv...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>[gun, gun, steal, line, distribution, post, ho...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "...                                                  ...     ...   \n",
       "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...      13   \n",
       "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...       4   \n",
       "11311  From: westes@netcom.com (Will Estes)\\nSubject:...       3   \n",
       "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...       1   \n",
       "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...       8   \n",
       "\n",
       "                   target_names  \\\n",
       "0                     rec.autos   \n",
       "1         comp.sys.mac.hardware   \n",
       "2         comp.sys.mac.hardware   \n",
       "3                 comp.graphics   \n",
       "4                     sci.space   \n",
       "...                         ...   \n",
       "11309                   sci.med   \n",
       "11310     comp.sys.mac.hardware   \n",
       "11311  comp.sys.ibm.pc.hardware   \n",
       "11312             comp.graphics   \n",
       "11313           rec.motorcycles   \n",
       "\n",
       "                                         data_lemmatized  predicted_topic  \n",
       "0      [thing, car, nntp, posting, host, line, wonder...               10  \n",
       "1      [final, summary, final, call, si, clock, repor...               10  \n",
       "2      [question, computer, network, distribution, li...               10  \n",
       "3      [division, line, distribution, world, nntp, po...               12  \n",
       "4      [article, owcb, world, std, com, tombaker, wor...               12  \n",
       "...                                                  ...              ...  \n",
       "11309  [factory, scan, distribution, factory, line, c...               10  \n",
       "11310  [problem, screen, blank, sometimes, minor, phy...               13  \n",
       "11311  [este, mount, case, line, instal, try, mount, ...               10  \n",
       "11312  [line, post, host, write, boy, embarasse, triv...               10  \n",
       "11313  [gun, gun, steal, line, distribution, post, ho...               10  \n",
       "\n",
       "[11314 rows x 5 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгруппируем все тексты по их предсказанным топикам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.groupby(['predicted_topic'])['data_lemmatized'].apply(lambda x: list(x.value_counts().index)).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем tf-idf представление слов внутри каждой подгруппы топиков и выведем топ-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ind = list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_dict = {}\n",
    "for i in d_ind:\n",
    "    cur_corpus = [' '.join(sent) for sent in d[i]]\n",
    "    X = vectorizer.fit_transform(cur_corpus)\n",
    "    sents = []\n",
    "    for j in range(len(X.toarray())):\n",
    "        ans = []\n",
    "        for nums in X.toarray()[j].argsort()[-5:]:\n",
    "            ans.append(vectorizer.get_feature_names()[nums])\n",
    "        sents.append(ans)\n",
    "    top5_dict[i] = sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датафрейм \"текст (выдаем списком, можно и не списком, но мне так проще) - определенный топик - топ-5 слов текста с самым высоким tf-idf в его группе топиков\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_text = []\n",
    "ans_topics =[]\n",
    "ans_top5 = []\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(d[i])):\n",
    "        ans_text.append(d[i][j])\n",
    "        ans_topics.append(i)\n",
    "        ans_top5.append(top5_dict[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans = pd.DataFrame.from_dict({'ans_text': ans_text,\n",
    "         'ans_topics': ans_topics,\n",
    "         'ans_top5': ans_top5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans.to_csv('NLP-HW-03.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание работы **Coherence Score**:\n",
    "\n",
    "**Определение:** Топик называется когерентным (согласованным), если термины, наиболее частые в данном топике, неслучайно часто совместно встречаются рядом в документах коллекции. \n",
    "Когерентность может оцениваться по сторонней коллекции, либо по той же коллекции, по которой строится модель(в нашем случае такой вариант).\n",
    "\n",
    "Для оценивания когерентности используется поточечная взаимная информация (pointwise mutual information, PMI):\n",
    "на примере двух тем А и В покажу, как это рассчитвается - \n",
    "логарифм (с любым основанием, на самом деле)((вероятность термина A встретиться вместе/рядом с термином B) / произведение априорных вероятностей появления термина А и B соответственно в обучающей выборке (отношение количества вхождений к общему количеству слов в корпусе))\n",
    "\n",
    "log(P(A near B)/P(A) * P(B))\n",
    "\n",
    "Примечание: (A near B) — вероятность термина A встретиться вместе/рядом с термином B; «рядом» можно конфигурировать вручную, по умолчанию расстояние равно 10 терминам влево и вправо; основание логарифма не играет роли, для простоты примем его равным 2. \n",
    "\n",
    "Позитивный знак логарифма будет означать положительны окрас A по сравнению с B, негативный — отрицательный. \n",
    "\n",
    "\n",
    "Например, чтобы определить к какому классу относятся высказывания «хорошая погода», «быстро ехать», достаточно проверить в обучающей выборке как часто «хорошая погода» и «быстро ехать» встречается рядом с заведомо (установленными человеком в зависимости от модели данных и тестовой выборки) хорошими и плохими словами и установить разницу. \n",
    "\n",
    "\n",
    "\n",
    "Литература:\n",
    "http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "\n",
    "\n",
    "на примере с водой очень понравилось:\n",
    "\n",
    "Segmentation: A lot of dispatch product divided into different sub-lot sizes, such that each sub-lot product are different.\n",
    "Probability Estimation: Quantitative Measurement of sub lot quality.\n",
    "Confirmation Measure: Determine quality as per some predefined standard (say % conformance) and assign some number to qualify. For example, 75% of products are good quality as per XXX standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
